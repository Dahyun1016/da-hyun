# Artificial neural network

Status: In progress
Created: 2023년 12월 6일 오후 2:25
복습: No

# 19차시 BAM을 활용한 패턴 연상

## BAM(Bidirectional Associative Memory)

- 이질 연상 메모리 : 입력 패턴과 연상된 출력패턴이 서로 다른 형태인 연상 메모리를 의미함
- 단층구조
- 입 , 출력층이 명확하게 구분되지 않은 X층과 Y층으로 구성
- 관련된 패턴쌍을 양방향으로 연상가능
- 왜곡된 패턴이 BAM에 입력될 경우 연상 메모리는 저장되어 있는 패턴쌍 중에서 입력 패턴과 가장 유사한 패턴의 대응 패턴을 연상 결과로 출력
- BAM에 기억된 패턴쌍 중에서 x층 (or y층) 으로 입력된 왜곡된 패턴과 가장 유사한 패턴의 대응 패턴을 출력함

---

# 20차시 자기조직화 지도

## 자율 신경망

### 자율 신경망 : 인간의 자율적인 학습과 유사한 형태로 학습이 이루어지는 신경망

- SOM
- ART

## 자기조직화 지도(Self-Organizing Map,SOM)

- 자율 신경망의 대표적인 모델
- 비지도 학습의 의한 클러스터링 방법 중의 하나
- 차원을 줄여서 가시화하는 방법 중의 하나
- 1980년대 튜보 코호넨에 의해서 고안됨

### SOM의 구조

- 입력층 , 출력층 구성 → 순반향 단층 신경망 구조
- 출력층 뉴런을 2차원으로 배열 → 사각형 배열 , 육각형 배열

### SOM의 학습 알고리즘

- SOM에서는 입력 패턴과 가장 유사한 가중치를 갖는 출력층 뉴런이 Winner 뉴런

1. 가중치 W 초기화
2. 가중치를 변경시킬 반경 r 설정 후 학습률 n 결정 , 최대 반복 횟수 결정
3. 입력 패턴 x와 가중치 사이의 유사도 D 계산 후 유사도가 가장 작은 뉴런을 위너 뉴런으로 선정
4. 위너 뉴런으로부터 임의의 반경 범위 내에 있는 뉴런들의 집합에 속해있는 뉴런들에 대해서 가중치 업데이트
5. 다음 입력 패턴을 입력하고 3,4 단계 과정을 수행 → 마지막 입력 패턴까지 위 과정을 반복 수행
6. 최대 반복 횟수에 도달하지 않을경우 반경r과 학습률 n을 감소 시키고 3단계로 이동하여 3~5 과정 반복 → 최대 반복 횟수에 도달시 학습 알고리즘 종료

---

# 21차시 SOM을 활용한 패턴 분류

## 패턴 분류 시나리오

SOM이 비지도 학습에 의한 클러스터링 방법 중 하나라고 알고있었음

비지도 학습 : 레이블 값이 존재하지 않는 상태에서 모델을 학습시키는 방법

### 클러스터링(Clustering)

SOM에서는 출력층 뉴런의 수만큼 클러스터를 형성 → 학습이 완료된 SOM은 입력 패턴과 가중치 사이의 유사성을 바탕으로 입력 패턴의 클래스를 결정

### 차원 축소 : 학습 데이터가 N 차원일 경우

SOM은 고차원 공간에서의 패턴들 사이의 유사성을 보존한 상태로 고차원의 데이터를 2차원 지도로 시각화 함

---

# 22차시 적응적 공명 이론

## 적응적 공명 이론(Adaptive Resonance Theory , ART)

**대표적인 자율 신경망 모델**

기존 신경망 모델의 문제점

- 학습이 완료된 상태( 가중치가 특정한 값으로 고정된 상태)에서 새로운 패턴을 학습시키고자 할 경우 처음부터 다시 신경망을 학습 시켜야했음

문제점 개선

- 기존 신경망 모델의 문제점을 개선하여 학습되지 않은 새로운 패턴이 들어올시 새로운 클러스터를 형성함으로써 이미 학습된 패턴들에 영향을 주지 않는 신경망 모델 → (적응적 공명 이론 ART)

### ART

- 1976 스티븐 그로스버그에 의해 처음 소개된 이후 다양한 적응적 공명이론 기술들이 개발 및 제안됨
- 자율 신경망의 대표적인 모델
- 비지도 학습에 의한 클러스터링 방법 중의 하나
- 스티븐 그로스버그 , 게일 카펜터에 의해서 고안

### ART의 구조

- 입력층 , 비교층 , 인식층 → 3계층 구조
- 비교층과 인식층 사이에 b와 t의 2가지 가중치를 사용하는것 특징
- 경계 파라미터를 사용하여 입력 패턴이 어떤 클러스터에 속하는지 판단

### ART의 학습 알고리즘

- 첫번째 입력 패턴을 첫 번째 클러스터의 대표 패턴으로 선정
- 다음 입력 패턴이 입력되면 첫 번째 대표 패턴과 비교
- if 첫번째 대표 패턴 거리가 임계값보다 작으면 첫 번째 클러스터로 분류되고 그렇지 않으면 새로운 클러스터를 생성
- 이 과정은 모든 입력 패턴들에 대해 적용 → 시간에 따라 점점 커질 수 있다!

- ART의 학습최종 가중치 값은 ㅇ ㅏ래 요인으로 크게 달라질 수 있음
1. 학습 패턴들을 입력하는 순서
- 경계 파라미터의 값
    - 경계 파라미터 값이 큼 → 입력 패턴 사이에 약간의 차이만 있어도 새로운 클러스터로 분류되기 쉬움
    
    - 경계 파라미터 값이 작음  → 입력 패턴과 저장된 패턴 사이에 많은 차이가 있더라도 기존 클러스터들 중 하나로 분류되기 쉬움
1. 클러스터의 수

1. 입력층 뉴런의 수가 n일 경우, 비교층에서 인식층으로의 가중치와 인식층에서 비교층으로의 가중치 초기화
2. 경계 파라미터 p의 값을 설정합니다.
3. 외부입력 s가 입력되면 , 비교층으로 보냅니다.
4. 인식층의 출력 y를 구하여 가장 큰 출력이 나오는 뉴런 j를 뉴런으로 선정
5. 위너 뉴런에 대하여 경계 파라미터를 사용하여 유사도 시험진행
6. 유사도 시험을 통과하면 위너 뉴런에 관련된 가중치를 업데이트
7. 위너 뉴런이 유사도 시험을 통과하지 못하면 새로운 위너 뉴런을 찾아 학습과정을 반복

---

# 23차시 패턴의 유사도

### 유사도(Similarity) 측정 도구

- 패턴들이 서로 관련이 있는지 없는지 얼마나 관련이 있는지 표현하는 측정 도구
- 유사도 측정 도구 : 해밍 거리(Hamming Distance), 유클리드 거리(Euclidean Distance)

### 경쟁식 신경망

- 패턴들 간의 유사도를 활용하여 학습하거나 응용하는 신경망
- 대표적인 신경망 모델 : Hamming Net , CP Net

### 해밍 거리 : 패턴들을 비교했을 때 값이 다른 비트들의 갯수 , 해밍 거리가 작을수록 유사함

### 유클리드 거리 : 패턴 공간에서 패턴들 간의 거리 , 유클리드 거리가 작을수록 유사함

---

# 24차시 Hamming Net

### Hamming Net

- 입력 패턴에 대하여 해밍 거리가 최소인 표본 패턴을 식별하는 신경망

### Hamming Net의 구조

- 순반향 단층 신경망 구조
- 입력 패턴이 n개의 픽셀 , 표본 패턴의 수 m → 입력층과 출력층은 각각 n , m 개의 뉴런들로 구성
- Hamming Net의 가중치 w는 별도의 학습을 하지 않고 표본 패턴 s로 부터 직접 구할 수 있음

### Hamming Net 응용

- Hamming Net은 활성화 함수를 경사 함수를 이용

입력 패턴과 가장 유사한 표본 패턴을 찾아내는 과정

→ 출력층 뉴런들 중에서 출력값이 가장 큰 뉴런의 색인(index) J를 찾음

→ Hamming Net에서는 입력패턴 x를 가장 유사한 표본패턴이라고 판단

---

# CP Net

### CP(Counter-propagation) Net

- 근사값 계산 , 패턴 분류 , 데이터 압축 등 여러 분야에서 응용되는 신경망

### CP Net의 구조

- CP Net은 순방향 다층 신경망 구조
- ‘입력층’ , ‘Kohonen층(은닉층)’ , ‘Grossberg층(출력층)’ 3개 구조

입력층 ↔ 은닉층 : 자율적인 학습을 하는 경쟁적인 뉴런 (자기 조직화 지도 : SOM )

은닉층↔ 출력층 : 은닉충과 완전 연결되어 있으니 (Fully - connected) 경쟁 x

### CP Net의 학습 알고리즘

1. 입력층↔ 은닉층 사이의 가중치 V와 은닉층↔출력층 사이 가중치 W 초기화
2. Instar 학습법의 학습률과 Outstar학습법 학습률 설정 , 최대 반복 횟수 결정
3. 입력 패턴 x와 은닉층 가중치 사이의 유사도 D계산 , D가 가장 작은 뉴런을 위너뉴런으로 선정
4. 입력층에서 은닉층 (위너)뉴런으로 들어가는 방향의 가중치들 업데이트
5. 다음 입력 패턴을 입력하고 3~4 과정 수행 , 마지막 입력 패턴까지 위 과정 반복 수행
6. 최대 반복 횟수에 도달하지 않았다면, 학습률을 감소 시킨 후 3단계로 이동하여 3~5 과정 반복 , 최대 반복 횟수에 도달 후 Instar 학습 과정 종료
7. 입력패턴 x와 은닉층 가중치 사이의 유사도 D를 계산하고 D가 가장 작은 뉴런을 위너뉴런으로 선정
8. 은닉층 뉴런에서 출력층 뉴런으로 나가는 방향의 가중치들 업데이트
9. 다음 입력 패턴을 입력하고 7~8 과정 수행 → 마지막 입력 패턴까지 위 과정 반복 수행
10. 최대 반복 횟수에 도달 x → 학습률 감소 시킨 다음 7단계로 이동하여 7~9 과정 반복 / 최대 반복 횟수 도달 → Outstar 학습 과정 종료

### CP Net에 있어서 은닉층의 위너 뉴런

- 가중치가 입력과 가장 유사한 뉴런
- 위너 뉴런 J 출력 : 1 → 나머지 뉴런들의 출력 : 0

### 학습률 (Learning Rate)

- 학습률 a,b 는 학습이 진행되는 동안 서서히 감소시켜서 보다 정확한 학습 이루어지게함

---

# 28차시 델타 학습법

## 신경망의 학습방법

### ① 지도 학습(Supervised Learning)
② 비지도 학습(Unsupervised Learning, 자율 학습)
③ 경쟁 학습(Competitive Learning)

### 오류 역전파 알고리즘(Backpropagation 알고리즘) - 델타 학습법

**델타 학습법 - 오류 역전파 알고리즘의 기본이 됨** 

- 1986년 T. McClelland와 D. Rumelhart가 고안한 신경망 학습 알고리즘
- 학습에 레이블과 신경망의 출력값뿐만 아니라 할성화 함수의 미분값이 사용되므로 **조건** 만족 해야함
- **조건**
1. 단조 증가 함수
2. 연속 함수
3. 미분 가능 함수
- 조건을 모두 만족하는 **시그모이드(Sigmoid)** 함수 일반적 사용

→ 단극성 시그모이드 함수 ↔ 양극성 시그모이드 함수

### 일반 델타 학습법

일반 델타 학습법 : 델타 학습법을 순방향 다층 신경망에 적용할 수 있게 확장한 학습방법

---

# 29차시 오류 역전파 알고리즘

### 오류 역전파 알고리즘 (Backpropagation , BP)

BP 알고리즘 : 일반적으로 순방향 다층 신경망인 다층 퍼셉트론(Mult-layer Perceptron,MLP)의 학습에 효과적으로 사용할 수 있어 가장 널리 사용되고 있는 학습 알고리즘

- BP 알고리즘은 출력층의 오차신호를 이용하여 은닉층과 출력층 사이의 가중치를 변경
- 또한 출력층의 오차 신호를 은닉층에 역전파하여 은닉층의 오차 신호를 구하고 이를 이용하여 입력층과 은닉층 사이의 가중치를 변경
- BP 알고리즘은 학습 단계의 역전파로 인해 순환 구조의 신경망이라고 오해할 수 있음 → **학습 과정에서만 오차에 관련된 출력이 역방향으로 전파**
- 학습이 완료되고 실제 응용시 **입력이 순방향으로 진행되면서 출력이 나오는 순방향 신경구조**라는것 유의!!

### BP 알고리즘을 이용한 신경망의 학습 3단계

1. 학습 패턴을 입력하여 출력 구함
2. 출력과 레이블 사이의 오차를 계산
3. 오차값을 역방향으로 전파시키면서 출력층의 가중치 및 은닉층의 가중치 변경

### BP 알고리즘의 학습 알고리즘

1. 입력층과 은닉층 사이의 가중치와 은닉층과 출력층 사이의 가중치 초기화
2. 학습 데이터로 사용할 p개의 패턴쌍(입력패턴,레이블) 선정
3. 적절한 학습률과 오차의 최대 한계치 결정
4. 가중치 업데이트하기 위해 학습 패턴쌍 차례로 입력
5. 은닉층의 입력 가중합을 계산한 다음 , 시그모이드 함수를 활성화 함수로 사용하여 출력 계산
6. 출력층의 입력 가중합을 계산한 다음. 시그모이드 함수를 활성화 함수로 사용하여 출력 계산
7. 레이블과 최종출력을 비교하여 오차 계산
8. 출력층의 오차 신호 계산
9. 은닉층에 전파되는 오차 신호 계산
10. 은닉층과 출력층 사이의 가중치 변화량을 계산하여 다음 학습 단계에서 사용될 가중치 계산
11. 입력층과 은닉층 사이의 가중치 변화량을 계산하여 다음 학습 단계에서 사용될 가중치 계산
12. 학습 패턴쌍을 반복 입력하여 가중치 업데이트
13. 오차 E가 오차 최대 한계치보다 작아지면 학습 종료

### 모멘텀 BP 알고리즘

BP 알고리즘을 이용한 신경망 학습의 문제점 : 은닉층↔ 출력층 사이의 가중치 변화량과 입력층↔출력층 사이의 가중치 변화량이 단지 학습률과 오차 신호에 의해 결정됨

→→→→→ 일반적으로 학습률 n을 작은 값으로 설정해 각 학습 단계에서의 가중치 변화량은 상대적으로 줄어들게 되므로 학습이 느려지는 현상 발생

- 모멘텀 BP 알고리즘은 이러한 문제점을 해결하기 위해 학습 단계에서 가중치를 업데이트 할 때 이 전 학습 단계의 가중치 변화량을 보조적으로 활용하는 방법

### 모멘텀 BP 알고리즘은 가중치 변화량들을 계산할 때 모멘텀 항이 부가된다는 점만 다르고 나머지는 BP 알고리즘과 동일

→ 이전 가중치 변화량 * 모멘텀 상수

---

# 30차시 학습 인자

### BP 알고리즘 . . . .의 장점 앤나 단점

- BP 알고리즘은 3계층 다층 신경망을 학습하는데 매우 유용
- 은닉층 수가 많아지면 학습이 느려짐 and 경우에 따라서 학습이 이루어 지지 않을 수도 있는 문제점 있음

—> 이런 문제점을 다양한 방법으로 해결함으로써 심층신경망 널리 사용될 수 있게 됨

### 매개 변수 - 가중치와 바이어스(Bias)

→ 매개변수 : 학습에 의해 최적의 값이 결정되는 가중치와 , 바이어스

신경망 학습에서는 매개 변수(Parameter)와 하이퍼 파라메터(Hyperparameter)라는 용어 사용

### 하이퍼 파라메터 : 매개 변수 이외에 학습을 효과적으로 수행하기 위해 설정해야 하는 많은 요소들

- 하이퍼 파라메터의 값은 학습에 큰 영향을 미치지만 주어진 상황에 따라 그 값이 제각각이기 때문에 어떤 값이 최적이라고 단정 x
- 대부분의 경우 , 많은 경험과 시행 착오를 통해 얻은 지식을 바탕으로 값 결정

### 초기 가중치

- 가중치의 초기화는 매우 중요함
- 초기 가중치는 적절한 값으로 설정
- if 초기 가중치 잘못 설정 → 응용 목적에 적합하게 학습 x
- 초기 가중치의 값은 너무 크지 않아야함 그렇다고 너무 작은 값도 x → 가중치 변화량이 매우 적게 되어 학습 시간이 오래걸림
- *** 주의 *** : 가중치의 값을 모두 0으로 초기화 하게 되면 학습이 진행되지 않음!

- 심층신경망(Deep Neural Network,DNN)의 경우 초기 가중치가 학습에 미치는 영향이 매우 크기 때문에 가중치의 초기화에 대한 연구가 다양하게 진행
- 오늘날에는 ’Xavier 초기화’ or ‘He 초기화’ 방법 사용
- Xavier 초기화 : 앞 계층의 뉴런 수가 n인 경우, 표준 편차가 1루트n 인 정규 분포로 가중치를 초기화하는 방법 (활성화 함수로 시그모이드 함수를 사용할 때 유용)
- He 초기화 : 표준 편차가 루트 2/n 인 정규 분포로 가중치를 초기화하며, 활성화 함수로 ReLU 함수를 사용할 때 학습 효과가 우수함

### 은닉층의 수

1. 은닉층의 수에 따라 신경망의 구조가 결정
2. 전체 가중치의 수가 결정
3. 은닉층의 수는 학습 시간에 상당한 영향을 줌

→ 은닉층의 수도, 신경망 학습에 있어 중요한 하이퍼 파라메터임

- 일반적으로 주어진 문제에 따라 입력층과 출력층 뉴런의 수는 직관적으로 산출 가능
1. 은닉층을 몇 계층으로 구성할 것인지
2. 각 은닉층에는 뉴런을 몇 개 배치할 것인지

→ 결정하기 어려움 , 시행착오를 통해 최적의 값을 찾아야함

### 비용 함수

신경망의 지도 학습에서는 레이블과 출력 차이인 오차를 줄여 가면서 가중치를 최적화 해가는 과정 반복

→ 오차가 작아지는 방향 , 즉 기울기가 감소하는 방향으로 학습 진행 

→ 경사 하강법(Gradient Descent)

- 비용 함수(Cost Function)은 손실 함수(Loss Function) 이라고도 하며 신경망의 오차와 밀접한 관련이 있음
- EXAMPLE : 신경망의 오차가 크면 비용 함수 증가 , 오차 작을시 비용함수도 감소

비용함수 (ㅇ ㅏ래의 2가지 형태가 많이 사용됨)

1. 평균 제곱 오차
2. 교차 엔트로피 오차

**평균 제곱 오차 (Mean Square Error , MSE)**

- 레이블과 출력의 차이인 오차를 제곱한것
- 오차가 큰 경우 학습이 빠르게 진행 , 오차가 작아질수록 학습 느리게 진행
- MSE 와 할성화 함수로 시그모이드 사용할 경우 → 시그모이드 함수를 미분하면 값이 매우 작아지기 때문에 경사 하강법으로 가중치를 업데이트 하기가 어려운 상황 발생

**교차 엔트로피 오차(Cross-entropy Error)**

- 교차 엔트로피 오차를 비용 함수로 사용하려면 출력 y과 0초과 1미만의 값 이어야함
- 활성화 함수로 시그모이드 함수나 소프트맥스 함수를 사용 할 수 있음

### 오버피팅(Overfitting)

오버피팅 : 과적합 → 신경망이 학습 데이터에 과도하게 특화되어 학습됨 → 학습에 사용된 데이터에 대해서는 성능이 좋지만 , **새로운 데이터에 대해서는 오히려 성능이 떨어지는 현상**

**언더피팅(Underfitting) : 학습이 너무 미진한 상황**

- 오버피팅은 일반적으로 학습 데이터가 너무 적어서 발생 → 기본적인 해결 방법은 학습 데이터의 수를 늘리는것
- 심층 신경망(DNN) 의 경우에는 은닉층의 수가 많기 때문에 가중치 수가 많아져 오버피팅이 발생하는 경우도 있음

### 오버피팅 해결방법

1. 정규화(Regularization)
- 오버피팅을 해결하기 위해 추가 정보를 도입
- 정규화는 특정 가중치가 너무 커져서 학습에 큰 영향을 미치는 현상을 방지
- 이를 위해 가중치를 작게하는 일종의 패널티 항목을 비용 함수에 추가
- 추가한 항목에 따라 ‘L1 정규화’ ‘L2 정규화’로 구분

1. 드롭아웃(Dropout)
- 오버피팅 문제를 해결하기 위해 입력층이나 은닉층의 일부 뉴런들을 삭제하고 해당 뉴런들에 연결된 가중치들을 학습에서 제외
- 학습에서 제외시킬 뉴런은 확률 P의 값에 의해 랜덤하게 선택
- 드롭아웃은 학습 단계에서만 적용하는것이고 실제 응용에선 모든 뉴런 사용